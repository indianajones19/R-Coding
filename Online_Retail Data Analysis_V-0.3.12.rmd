---
title: "Forcasting Revenue for Online Retail Store"
author: "Muhammad Rahman"
date: "April 5, 2020"
output: rmd file
Data Source: https://archive.ics.uci.edu/ml/datasets/online+retail
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Initial Data Analysis:

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)

setwd("H:/Ryerson Data Science and Analytics/06. CKME 136 - Capstone Project/MSR Capstone Project/DataSet from UCI and Code Capstone")

RawRetailData <- read.csv(file= "Online_Retail.csv", header = TRUE, sep = ",", dec = ",", fill = TRUE, comment.char = "")

View (RawRetailData)

str (RawRetailData)

sum(is.na(RawRetailData)) ## Checking NA. 135080 found

colSums(is.na(RawRetailData)) ## Checking which column has how many NA. All NA found in CustomerID.

sapply(RawRetailData, function(x) length(unique(x))) ## Checking unique number of items of each column.

## Note: we have 135,080 missing value in CustomerID leaving us with 406,829 entries with CustomerID. There are 4373 unique Customers among 406,829 CustomerIDs. We assume these missing values represents a number of guest shoppers unless there are really missing informaiton here. However, even if we do not consider CustomerID, our analysis can be done leveraging rest of the data points. Let's move on.

## We have 38 unique Country. Decided to keep this value as factor.

## We need to know sales informaiton. Therefore, Quantitiy and UniPrice will be two important data. We will convert both of them to numeric, Quantity as Integer and UnitPrice as Float. Uniqe value is not considerable here.

## There is StockCode and Description of the item representing products sold. we will consider only StockCode.

## Further analysis revealed, IvoiceNo will be the best to consider in terms of identifing sales since each sale must have an InvoiceNo and there is no missing data. 

## Now that we are good with InvoiceNo to identify each sale and determine actual monetary value of each sale by multiplying Quantity with UnitPrice and cll it Sales. We are good to go with analysis of time series data. By the way, please consider there are 25,900 unique InvoiceNo indicating customer purchasing from 38 different counteis. 

## We will take InvoiceDate and split it into SalesDate and SalesTime. Furhter we will extract DayOfWeek, Month, and Year. Finally we will determine Sale Status by analysing Sales value. Positive values indicate Sold and Negetive valuse indicating Returned. 

```
# ======= R-Studio Environment Setup =======

```{r Create work environment: install packages, call library functions etc.}

# Function to install necessary R packages:

install <- function(packages)
{
  new.packages <- packages[!(packages %in% installed.packages()[, "Package"])]
  if (length(new.packages)) 
    install.packages(new.packages, dependencies = TRUE)
  sapply(packages, require, character.only = TRUE)
}

# Necessary packages to install:

necessary_packages <- c("caret", "tidyquant", "tidyverse", "timetk", "broom", "ggcorrplot", "modelr", "matrixStats",  "gridExtra", "grid")

install(necessary_packages)

options(na.action = na.warn)

```

Package/key functions description:

caret: The caret package (short for Classification And REgression Training) is a set of functions that attempt to streamline the process for creating predictive models. The package contains tools for: data splitting. pre-processing

predict: The prediction and margins packages are a combined effort to port the functionality of Stata's (closed source) margins command to (open source) R. prediction is focused on one function - prediction() - that provides type-safe methods for generating predictions from fitted regression models.

tidyquant: Tidy Quantitative Financial Analysis: This R package brings business and financial analysis to the 'tidyverse'. The 'tidyquant' package provides a convenient wrapper to various 'xts', 'zoo', 'quantmod', 'TTR' and 'PerformanceAnalytics' package functions and returns the objects in the tidy 'tibble' format. The main advantage is being able to use quantitative functions with the 'tidyverse' functions including 'purrr', 'dplyr', 'tidyr', 'ggplot2', 'lubridate', etc.

tidyverse: The tidyverse is an opinionated collection of R packages designed for data science. All packages share an underlying design philosophy, grammar, and data structures. It can be observed, how the tidyverse makes data science faster, easier and more fun with “R for Data Science”

timetk: A Tool Kit for Working with Time Series in R. timetk is used for easy visualization, wrangling, and preprocessing of time series data for forecasting and machine learning prediction.

modelr:  Modeling Functions that Work with the Pipe. This is a set of functions for modeling that helps seamlessly integrate modeling into a pipeline of data manipulation and visualization. This set of functions imports ‘broom’, ‘magrittr’, ‘purrr’, ‘rlang’, ‘tibble’, ‘tidyr’, ‘tidyselect’, ‘vctrs’. 

broom: broom is a package for tidying statistical models into data frames

ggcorrplot: This package is used to visualize easily a correlation matrix using 'ggplot2'. It provides a solution for reordering the correlation matrix and displays the significance level on the plot. It also includes a function for computing a matrix of correlation p-values.

matrixStats: Functions that Apply to Rows and Columns of Matrices (and to Vectors). matrixStats is a high-performing functions operating on rows and columns of matrices, e.g. col / rowMedians(), col / rowRanks(), and col / rowSds(). Functions optimized per data type and for subsetted calculations such that both memory usage and processing time is minimized. There are also optimized vector-based methods, e.g. binMeans(), madDiff() and weightedMedian().

gridExtra: Miscellaneous Functions for "Grid" Graphics
Provides a number of user-level functions to work with "grid" graphics, notably to arrange multiple grid-based plots on a page, and draw tables.

grid: An R function to add grid to graphs.

# Read and load data from given dataset as planned. Dataset Source: https://archive.ics.uci.edu/ml/datasets/online+retail

```{r load retail dataset, echo=FALSE}
RetailData <- read_csv("Online_Retail.csv",
                   col_types = cols(
                      InvoiceNo = col_character(),
                      StockCode = col_character(),
                      Description = col_character(),
                      Quantity = col_integer(),
                      InvoiceDate = col_datetime("%m/%d/%Y %H:%M"),
                      UnitPrice = col_double(),
                      CustomerID = col_integer(),
                      Country = col_character()
                      ))

View(RetailData)
dim(RetailData)
# 541909 Rows and 8 Columns

RetailData <- mutate(RetailData, SalesDate = parse_date(format(InvoiceDate, "%Y-%m-%d")),
         WeekDay = wday(SalesDate, label = TRUE),
         SalesTime = parse_time(format(InvoiceDate, "%H:%M")),
         Month = format(InvoiceDate, "%m"),
         Sales = Quantity * UnitPrice,
         SoldReturned = ifelse(Quantity > 0, "Sold", "Returned"))

dim(RetailData)
# 541909 rows and 14 Columns
```

# Let us analyze this RetailData after mutaion. We used mutate() function to split date/time data and added sales information. 

```{r}
View(RetailData)

write.csv(RetailData, "NewRetailData.csv")

str(RetailData)
```

Initial analysis reveals:
Total No or Rows: 541909
There is no NA value in any columnn except in CustomerId
Total NA value in CustomerId = 135080
This leaves 541909 - 135080 = 406829 Rows of data.

We could not find any logic to impute the CustomerId value in the dataset. There are transaction data for other datafields where CustomerId is missing. this could be the result of guest purchases.
We can assume a number for guest and impute CustomerId field - Ask Professor.
Otherwise we have 406829 rows of data for our analysis.

Now it is time to get missing and unique values in each columns considered for data analysis.

```{r checking missing value }
colSums(is.na(RetailData))
```

# Above code result shows that, there are 1454 missing value in Description and 135080 in CustomerID column. We discussed about CustomerID before. As we see StockCode is correspondent for Description and StockCode does not have any missing value, we will consider StockCode in our Analysis and ignore Description.


# Exploratory Data Analysis:


```{r Unique items in Column 1, 2, 7 & 8, meaning column: InvoiceNo, StockCode, CustomerId and Country }
sapply(RetailData[,c(1, 2, 7, 8, 9)], function(x)length(unique(x)))
```

We have 25900 unique InvoiceNo,  4070 unique StockCode, 4373 unique CustomerID, 38 unique Country and 305 unique SalesDate in the dataset out of a total of 541909 records.

Therefore, we can conclude:

There are 25900 orders placed where there were multiple items in the order. These items are one or more representing the 4070 StockCodes.

4373 Uniques customers placed these orders from 38 different countries. means, many of these customers are repeatign customers placed variety of orders combination of multiple items in each order.

Obviously, these repeating customers placed their orders in different time of the year.

305 unique SalesDate tells us on an average, on a given day 541909/305 = 1777 transactions happened in the store. If we can somehow consolidate and/or analyze data for each day and perform our Data Analysis, this will give us opportunity to work on a smaller dataset with greater analytics and forecasting capability. However, we have to get a clear picture for each days' transactional data.


```{r Let's start with Country. Name of countries:  }
unique(RetailData$Country)

# Since this data is a UK based online retail store, possibility is there that maximum sales may come from UK. Let's check it out.

sum(RetailData$Country == "United Kingdom")

# 495478 records out of 541909 rows found for United Kingdom
```

# Let's see how many of each item sold. We will see first 6 records as sample.
```{r Number of items sold}
RetailData %>%
  group_by(StockCode, Description) %>%
  summarise(TotalQuantity = sum(Quantity)) %>%
  arrange(-TotalQuantity) %>% head()
```

# Let us plot country wide selling trend. We will keep United Kingdom separate from the rest.

```{r Courntry wise selling trend. Plot1 is only for United Kingdom and Plot2 is for rest }

UKSales <- RetailData %>%
  filter(Country == "United Kingdom") %>%
ggplot(aes(x = Country, fill = SoldReturned)) + geom_bar(alpha = 0.8) + scale_fill_grey(start = 0.8, end = 0.2, na.value = 'red', aesthetics = "fill") + theme_tq_green() 

OtherSales <- RetailData %>%
  filter(Country != "United Kingdom") %>%
  ggplot(aes(x = Country, fill = SoldReturned)) + geom_bar(alpha = 0.8) + scale_fill_grey(start = 0.8, end = 0.2, na.value = 'red', aesthetics = "fill") + theme_tq_green() + theme(axis.text.x = element_text(angle = 90, vjust = 1, hjust = 1)) + labs(y = "No. of Items Sold/Returned")

title <- textGrob("UK and Other Countries - Sales trend", gp = gpar(fontface = "bold", cex = 1.5))

grid.arrange(UKSales, OtherSales, top=title, widths = c(0.2, 1.2))

```

```{r SoldReturned, Sold / Returned Analysis over time}

RetailData %>%
ggplot(aes(x = SalesDate, color = SoldReturned)) + ggtitle("Sell/Returns over time") + 
    theme(plot.title = element_text(hjust = 0.5))+ 
    facet_grid(SoldReturned ~., scales = "free") + geom_freqpoly(bins = 90, size = 1, alpha = 1) +
    scale_color_tq(theme = "green") + labs(y = "Sell/Return Count")
```

# Purchases over the time of the day
```{r Sales over time during the day, compared over time}
RetailData %>%
ggplot(aes(x = SalesTime, y = SalesDate)) + ggtitle("Trend of Transaction over time in a day") +   theme(plot.title = element_text(hjust = 0.5))+
stat_bin_2d(alpha = 1, bins = 20, color = "white") +
scale_fill_gradientn(colours = c(palette_light()[[1]], palette_light()[[2]])) 

```

# Sales per day over the months of the year
```{r Sales per day over Month}
RetailData %>%
  mutate(day = format(InvoiceDate, "%d")) %>%
  group_by(Month, day) %>%
  summarise(Sales = sum(Sales)) %>%
  ggplot(aes(x = Month, y = day, fill = Sales)) + ggtitle("Each Day Sales Over Month") +   theme(plot.title = element_text(hjust = 0.5))+ theme(legend.position = "right") + ylab("Days of the Month")+ xlab("Months") + geom_tile(alpha = 1, color = "white") + scale_fill_gradientn(colours = c(palette_light()[[1]], palette_light()[[2]]))

```
# From January to December, the graph above shows sales of each day of the Month. Overall sales grew uniformly throuout the year and took a spike twoards the end the year. The final quarter was good.
 
# Let's revisit quantities of item sold.
```{r Number of items sold}
RetailData %>%
  group_by(StockCode, Description) %>%
  summarise(TotalQuantity = sum(Quantity)) %>%
  arrange(-TotalQuantity) %>% head()

```
# As we see from above code result, different items (as per StockCode) is sold in different quantity. We like to see them in groups. Such as sold below 10,000, in between 10,000 to 50,000, more than 50,000.

# 
```{r Sum of all Quantity }

# sum of Quantitiy including returns

p1 <- RetailData %>%
  group_by(StockCode, Description) %>%
  summarise(TotalQuantity = sum(Quantity)) %>%
  ggplot(aes(x = TotalQuantity)) + 
              geom_density(fill = palette_light()[[1]], alpha = 1) + theme_tq() + xlab("Items with Returns") + ylab(" ")

# Sum of Quantitiy without returns 

p2 <- RetailData %>%
  group_by(StockCode, Description) %>%
  summarise(TotalQuantity = sum(Quantity)) %>%
  filter(TotalQuantity > 1) %>%
  ggplot(aes(x = TotalQuantity)) + 
              geom_density(fill = palette_light()[[1]], alpha = 1) + theme_tq() + xlab("Items Sold") + ylab(" ")

# Sum of Quantitiy with high volume ( > 10000)

p3 <- RetailData %>%
  group_by(StockCode, Description) %>%
  summarise(TotalQuantity = sum(Quantity)) %>%
  filter(TotalQuantity > 10000) %>%
  ggplot(aes(x = TotalQuantity)) + 
              geom_density(fill = palette_light()[[1]], alpha = 1) + theme_tq() + xlab("Items Sold with High Volume") + ylab(" ")

title1 <- textGrob("Quantity wise product sales", gp = gpar(fontface = "bold", cex = 1.5))    
grid.arrange(p1, p2, p3, ncol = 3,top=title1 ,widths = c(0.6, 0.5, 0.7))

```

# For Business, it is important to know What are the best items in store. Such as items sold in highest quantity at fastest pace versus items sold at lowest quantity or may be slowest moving itmes. For our analysis we will only find to Sales and Transaction and discurd the low volume slow moving items.

```{r}
TopSellingItems <- RetailData %>%
  group_by(SalesDate, StockCode, Description) %>%
  summarise(sum = sum(Quantity)) %>%
  group_by(StockCode, Description)%>%
  summarise(n = n()) %>%
  arrange(-n)

TopTransactionItems <- RetailData %>%
  filter(StockCode %in% TopSellingItems$StockCode[1:5]) %>%
  group_by(SalesDate, StockCode) %>%
  summarise(sum = sum(Quantity)) %>%
  spread(key = StockCode, value = sum)

RetailData %>%
  filter(StockCode %in% TopSellingItems$StockCode[1:5]) %>%
  group_by(SalesDate, StockCode, Description) %>%
  summarise(sum = sum(Quantity)) %>%
    ggplot(aes(x = SalesDate, y = sum)) +
    facet_wrap(~ StockCode, ncol = 1, scales = "free") +
    geom_line(color = palette_dark()[[6]], size = 1, alpha = 0.8) +
    theme_tq() +
    labs(Title = "Five Topmost Items Sold by Quantity", x = "Five Topmost Items Selling Trend", 
         y = "Sold Item Quantity")

```

# The most sold item is "WHITE HANGING HEART T-LIGHT HOLDER" with StockCode 85123A. It is sold 304 times. Which means almost every day this item was sold from the store. Let's analyze the sales trend of only this item:

```{r }
RetailData %>%
  filter(StockCode == "85123A") %>%
  group_by(SalesDate, SoldReturned) %>%
  summarise(HighestSellingItem = sum(Quantity)) %>%
  ggplot(aes(x = SalesDate, y = HighestSellingItem, color = SoldReturned)) + ggtitle("Highest selling item's transaction record") +   theme(plot.title = element_text(hjust = 0.5))+ ylab("Quantities")+
    facet_wrap(~ SoldReturned, ncol = 1, scales = "free") +
    geom_line(size = .4, alpha = 1) + 
    scale_color_tq(values = palette_dark()) 
    
```

# For any business, the central point of success is customer. From business point of view, an entreprenure must take utmost care of his/her loyal customers. Let's see how many recurring customer are there. To start with, we will analyze frequency of transactions done by customers on our sales data.

```{r Recurring/Repeating and One Time Customers}

TransactionsFrequency <- RetailData %>%
  group_by(SalesDate, CustomerID) %>%
  summarise(sum = sum(Quantity)) %>%
  group_by(CustomerID) %>%
  summarise(n = n()) %>%
  mutate(CustomerType = ifelse(n > 1, "RepeatingCustomers", "OneTimeCustomer"))

TransactionsFrequency 

# Shows a tabl of CustomerID & No of transaction (n) at first. n = 1 is One Time and n > 1 Repeating Customer. 

length(which(TransactionsFrequency$CustomerType == "RepeatingCustomers")) 

# There are 2992 repeating customers in the store.
```

# Remember, we need to concentrate on datewise/daily analysis. At this point, we need to see how many one time and how many repeating customers visited the store on a given day and we will analyze further from there.

```{r Repeating and One Time Customers }

CustomerFrequency <- left_join(RetailData, TransactionsFrequency, by = "CustomerID") %>%
  distinct(SalesDate, CustomerID, CustomerType) %>%
  group_by(SalesDate, CustomerType) %>%
  summarise(n = n()) %>%
  spread(key = CustomerType, value = n)

CustomerFrequency

TransactionsFrequency %>%
  group_by(CustomerType) %>%
  summarise(n = n()) %>%
  mutate(Ratio = n / sum(n)) %>%
  ggplot(aes(x = "", y = Ratio, fill = CustomerType)) +ggtitle("One time and repeating customer ratio") +   theme(plot.title = element_text(hjust = 0.5))+ ylab("")+ xlab("")+
    geom_bar(stat = "identity", alpha = 1) +
    coord_polar("y", start = 0) +
    scale_fill_tq(values = palette_dark()) 
    
```

# We need to know daywise how many items sold, their mean, what is daily sales total and mean sales.

```{r Mean items sold, mean quantitity sold per customer and mean Sales per customer on daily basis}

purchases <- RetailData %>%
  group_by(SalesDate, CustomerID) %>%
  summarise(n = n(),
            TotalQuantity = sum(Quantity),
            TotalSales = sum(Sales)) %>%
  group_by(SalesDate) %>% 
# Upto this much, what we get is, on a give day (say, 01Dec2010), a customer(12431) purchased 14 different items(as per 14 different StockCode) whose total Quantity is 107 (e.g. he purchased 6 of StockCode 22941, 8 of StockCode 21622 etc.). This is TotalQuantity = 107. TotalSales is the sum of Sales for those 107 Quantites/items purchased (in this case it is 358).
  summarise(MeanSalesPerCustomer = mean(TotalSales),
            MeanQuantityPerCustomer = mean(TotalQuantity),
            MeanOrdersPerCustomer = mean(n))
purchases
# Now that in summary, using summarise() function, we agregrated these values as explained below. Please note that, all data at this point is grouped by SalesDate. Meaning, each of the following variables represetn data for one day of the year and this will be our input to datamodel file.

# MeanSalesPerCustomer: (sum/total Sales made to each customer on a given day) / (no of unique customer)
# MeanQuantityPerCustomer: (sum/total Quantity/items sold to each customer on a given day) / (no of unique customer)
# MeanOrdersPerCustomer: (sum/total number of Quantity sold to each customer[total number of order placed by customer on a given day] on a given day) / (no of unique customer)

# Visualize:

purchases %>%
  gather(x, y, MeanSalesPerCustomer:MeanOrdersPerCustomer) %>%
  ggplot(aes(x = SalesDate, y = y)) + ggtitle("Sales trend by each customer") + theme(plot.title = element_text(hjust = 0.5))+ ylab("")+ xlab("")+
    facet_wrap(~ x, ncol = 1, scales = "free") +
    geom_line(color = palette_light()[[1]], size = 1, alpha = 1) +
    geom_smooth(color = palette_light()[[2]], method = 'loess') 
    
```

#  Let's analyze the Sold and Returned itme patterns

```{r Sold Returned items analysis (on daily basis grouped by SalesDate)}

ItemsSoldReturned <- RetailData %>%
  group_by(SalesDate, SoldReturned) %>%
  summarise(TotalQuantity = sum(Quantity)) %>%
  spread(key = SoldReturned, value = TotalQuantity)

ItemsSoldReturned

ItemsSoldReturned%>%
  gather(x, y, Sold:Returned) %>%
  ggplot(aes(x = SalesDate, y = y, color = x)) + 
    geom_line(size = 1, alpha = 0.8) +
    theme_tq() +
    labs(x = "", 
         y = "Items Quantity",
         title = "Items Sold and Returned over time")

```

# How many different items are purchased/returned per day?

```{r transactions per day: purchased and returned both}

TransactedItems <- RetailData %>%
  group_by(SalesDate, StockCode) %>%
  summarise(n = n()) %>%
  group_by(SalesDate) %>%
  summarise(TransactedItems = n())

TransactedItems

# This is telling us how many unique items (as per StockCode) were transacted (Sold and/or Returned)

TransactedItems %>%
  ggplot(aes(x = SalesDate, y = TransactedItems)) +
    geom_line(color = palette_light()[[6]], size = 1, alpha = 0.8) +
    geom_smooth(color = palette_light()[[2]], method = 'loess') +
    theme_tq() +
    labs(title= "Items Transacted (Sold and/or Returned) Over the year", x = "Quarters/Seasons", 
         y = "Number of Items",
         color = "")
```

```{r Calculate Net Sales: Total, mean Sales per day and total, mean Quantity sold/returned per day}

NetSales <- RetailData %>%
  group_by(SalesDate) %>%
  summarise(TotalSalesPerDay = sum(Sales),
            MeanSalesPerDay = mean(Sales), 
            TotalQuantitySoldPerDay = sum(Quantity),
            MeanQuantitySoldPerDay = mean(Quantity))

NetSales

# NetSales: Total Sales (Income) from both Sold and Returned items per day
# TotalSalesPerDay: Summation of Sales on a given day
# MeanSalesPerDay: Summation of Sales on a given day / Total items sold andor returned (Total itmes of StockCode wise totals on a given day)
# TotalQuantitySoldPerDay: Summation of Quantity sold andor returned on a given day 
# MeanQuantitySoldPerDay: Summation of Quantity sold andor returned on a given day / Total items sold andor returned (Total itmes of StockCode wise totals on a given day)

NetSales %>%
  gather(x, y, TotalSalesPerDay : MeanQuantitySoldPerDay) %>%
  ggplot(aes(x = SalesDate, y = y)) +
    facet_wrap(~ x, ncol = 1, scales = "free") +
    geom_line(color = palette_light()[[1]], size = 1, alpha = 0.8) +
    geom_smooth(color = palette_light()[[2]], method = 'loess') +
    theme_tq() +
    labs(title="Mean and Sum of Sales and Quantity", x = "Quarters/Seasons", 
         y = "Number / Volume of Items")
```


# Revenue from Sales and Quantity analysis (Only sold items to consider and analyze)
```{r }
Revenue <- RetailData %>% # Revenue = sum of sales of items
  filter(Sales > 0) %>%
  group_by(SalesDate) %>%
  summarise(TotalRevenueFromSales = sum(Sales), 
            MeanRevenueFromSales = mean(Sales),
            QuantitySoldForRevenue = sum(Quantity),
            MeanQuantitySoldForRevenue = mean(Quantity))

# IMPORTANT: DONT CHANGE "TotalRevenueFromSales" TO 'revenue' here. It is changed in "TimeseriesTransaction". Line: 619

Revenue

Revenue %>%
  gather(x, y, TotalRevenueFromSales:MeanQuantitySoldForRevenue) %>%
  ggplot(aes(x = SalesDate, y = y)) +
    facet_wrap(~ x, ncol = 1, scales = "free") +
    geom_line(color = palette_light()[[1]], size = 1, alpha = 0.8) +
    geom_smooth(color = palette_light()[[2]], method = 'loess') +
    theme_tq() +
    labs(title= "Revenue from Sales and Quantity sold over the year", x = "Quarters/Seasons", 
         y = "Number / Volume of Items")
```


# Sales analysis on Returned Items (Sales Returned - Refund paid back to customer) including Quantity Returned analysis
```{r}
SalesReturned <- RetailData %>% # Sales Returned - Refund paid back to customer
  filter(Sales < 0) %>%
  group_by(SalesDate) %>%
  summarise(TotalSalesReturned = sum(Sales),
            MeanSalesReturned = mean(Sales),
            TotalQuantityReturned = sum(Quantity),
            MeanQuantityReturned = mean(Quantity))

SalesReturned

# Visualize SalesReturned:

SalesReturned %>%
  gather(x, y, TotalSalesReturned:MeanQuantityReturned) %>%
  ggplot(aes(x = SalesDate, y = y)) +
    facet_wrap(~ x, ncol = 1, scales = "free") +
    geom_line(color = palette_light()[[2]], size = 1, alpha = 0.8) +
    theme_tq() +
    labs(title= "SalesReturned from Sales and Quantity returned over the year", x = "Quarters/Seasons", y = "Number / Volume of Items")
```

```{r Arrange and prepare Data for modeling}

# We have UnitPrice. We need to know "Unit-Quantity". Our target is to find out what are the items were sold (as per StockCode) and how many of them were sold each day. Once we get this, we can get mean of each items sold  By Unit-Quantity, we mean: 


UnitQuantity <- distinct(select(RetailData, SalesDate, StockCode, UnitPrice)) %>% # selecting distinct SalesDate, StockCode and UnitPrice.
  mutate(UnitQuantity = paste(SalesDate, StockCode, sep = "_")) %>% # using mutate() pasting SalesDate and StockCode with underscore and calling it UnitQuantity, 
  select(UnitQuantity, UnitPrice) # projecting UnitQuantity with UnitPrice. This will help us calcualting mean UnitPrice against StockCode.

head(UnitQuantity)

UnitQuantity <- distinct(select(RetailData, SalesDate, StockCode, UnitPrice)) %>% 
  mutate(UnitQuantity = paste(SalesDate, StockCode, sep = "_")) %>% 
  select(UnitQuantity, UnitPrice)

head(UnitQuantity)
dim(UnitQuantity)
# 344928 Rows 2 Columns

MeanPricePerUnit <- RetailData %>%
  filter(SoldReturned == "Sold") %>%
  group_by(SalesDate, StockCode) %>%
  summarise(n = n()) %>%
  mutate(UnitQuantity = paste(SalesDate, StockCode, sep = "_")) %>%
  left_join(UnitQuantity, by = "UnitQuantity") %>% # adding UnitQuantity table
  group_by(SalesDate, StockCode) %>%
  summarise(mean = mean(UnitPrice)) %>%
  group_by(SalesDate) %>%
  summarise(MeanPricePerUnit = mean(mean))

head(MeanPricePerUnit)
dim(MeanPricePerUnit)
# 305 Rows and 2 Columns. Recall that, 305 is the Total number of Unique days in initial "RetailData" dataset.

MeanPricePerUnit %>%
     ggplot(aes(x = SalesDate, y = MeanPricePerUnit)) +
     geom_line(color = palette_light()[[1]], size = 0.8, alpha = 1) +
     theme_tq_green() +
     labs(title = "MeanPricePerUnit changes over the year", x = "Quarters/Seasons", 
          y = "Mean Price Per Unit")

```

# Forcasting data preparation

```{r Putting it all together for forcasting data preparation}

DailyTransaction <- distinct(select(RetailData, SalesDate, WeekDay, Month)) %>%
  left_join(NetSales, by = "SalesDate") %>%
  left_join(MeanPricePerUnit, by = "SalesDate") %>%
  left_join(Revenue, by = "SalesDate") %>%
  left_join(SalesReturned, by = "SalesDate") %>%
  left_join(purchases, by = "SalesDate") %>%
  left_join(CustomerFrequency, by = "SalesDate") %>%
  left_join(ItemsSoldReturned, by = "SalesDate") %>%
  left_join(TransactedItems, by = "SalesDate") %>%
  left_join(TopTransactionItems, by = "SalesDate") %>%
  mutate(DifferenceInTotalRevenue = TotalRevenueFromSales - lag(TotalRevenueFromSales),
         Season = ifelse(Month %in% c("03", "04", "05"), "Spring",
                         ifelse(Month %in% c("06", "07", "08"), "Summer",
                                ifelse(Month %in% c("09", "10", "11"), "Fall", "Winter"))))

dim(DailyTransaction)
# 305 Rows and 31 Columns
View(DailyTransaction)

# Let's add P_ with the five top selling product codes

colnames(DailyTransaction)[grep("^[0-9]+", colnames(DailyTransaction))] <- paste0("P_", colnames(DailyTransaction)[grep("^[0-9]+", colnames(DailyTransaction))])

dim(DailyTransaction)
# 305 Rows 31 Columns. Row 1 has one NA value. We will work on Dimentionality reduction.
```

```{r Split Data into train and test dataset. SalesDate before Oct2011}
DailyTransaction <- DailyTransaction %>%
  mutate(model = ifelse(SalesDate <= "2011-10-01", "train", "test"))

```
# =================== Submitted up to this much as Initial Code Submission ==============================

```{r Visualization Train and Test data}

DailyTransaction %>%
  ggplot(aes(x = SalesDate, y = TotalRevenueFromSales, color = model)) +
    geom_point(alpha = 0.5) +
    geom_line(alpha = 0.5) +
    #labs(title = "Training/Test Data Segregation", x = "Quarters", y = "Revenue")
    theme_tq()
```

```{r Creating Time Series Data for revenue. We can compare many other columns as such. At this point we are considering Revenue as primary attribute to generate timeseries data and analyse it.}

TimeseriesTransaction <- DailyTransaction %>%
  rename(date = SalesDate) %>%
  rename(revenue = TotalRevenueFromSales) %>%
  select(model, date, revenue) %>% 
  tk_augment_timeseries_signature() %>%
  select(-contains("Month"))


# tk_augment_timeseries_signature(.data, .date_var = NULL): Add many time series features to the data. 
# .data: A time-based tibble or time-series object.
# .date_var: For tibbles, a column containing either date or date-time values. If NULL, the time-based column will interpret from the object (tibble, xts, zoo, etc)
# tk_augment_timeseries_signature() adds 25+ time series features including:
# Trend in Seconds Granularity: index.num
# Yearly Seasonality: Year, Month, Quarter
# Weekly Seasonality: Week of Month, Day of Month, Day of Week, and more
# Daily Seasonality: Hour, Minute, Second
# Weekly Cyclic Patterns: 2 weeks, 3 weeks, 4 weeks

dim(TimeseriesTransaction)
# 305 28

# At this point our target is to reduce dimension as much as possible and only keep necessary data for our analysis.

# First step: Remove NA value:

TimeseriesTransaction <- TimeseriesTransaction[complete.cases(TimeseriesTransaction), ]

# complete.cases(): Return a logical vector indicating which cases are complete, i.e., have no missing (NA) values 

dim(TimeseriesTransaction)
# 304 28, Top row deleted due to NA value in diff column. diff = index.number - lag(index.number)
```

```{r Now we will target finding zero Variance in data and remove them.}

(var <- data.frame(
          colnames = colnames(TimeseriesTransaction[, sapply(TimeseriesTransaction, is.numeric)]),
          colvars = colVars(as.matrix(TimeseriesTransaction[, sapply(TimeseriesTransaction, is.numeric)]))) %>%
  filter(colvars == 0))

# colVars(): Variance estimates for each row (column) in a matrix.
# Identifying the columns having single value resulting no Variance. i.e. Variance = 0. We found 5 such columns. Namely, hour, minute, second, hour12, am.pm. These columns will not add any value to our analysis. Discarding 
```

```{r Removing any character value from data using one_of() function}

TimeseriesTransaction <- select(TimeseriesTransaction, -one_of(as.character(var$colnames)))

```

# Now we will detect correlation amoung variables and remove highly correlated features

```{r To get Correlation, we need only numeric values from dataset}

# First using sapply() creating vector TimeseriesTransaction and then finding correlation among them. Storing in variable 'relation'

relation <- cor(TimeseriesTransaction[, sapply(TimeseriesTransaction, is.numeric)])

# let's compute p-value for correlation:

p.relation <- cor_pmat(TimeseriesTransaction[, sapply(TimeseriesTransaction, is.numeric)])

# cor(): Compute correlation between numeric variables.
# cor_pmat(): Compute a correlation matrix p-values.

ggcorrplot(relation, p.mat = p.relation, hc.order = TRUE, type = "lower", outline.col = "white",  
           colors = c(palette_light()[1], "grey", palette_light()[2]))

# Explaination: Positive Correlation is strong represented by Red. Strong negetive correlation is represente by dark grey. Squares with cross mark are not considered as correlated. which is somewhere near value 0.0 and represented by very light grey/white shade. 

# ggcorrplot(relation,  type = "upper", outline.col = "white", hc.order = TRUE, p.mat = p.relation, colors = c(palette_light()[1], "white", palette_light()[2]))
```

```{r Finding Highly Correlated values}

correlation <- findCorrelation(relation, cutoff=0.8) # Not Considering any value less that 0.8
TimeseriesTransaction <- select(TimeseriesTransaction, -one_of(colnames(relation)[correlation]))

dim(TimeseriesTransaction)
# 304 Rows 15 Columns. 

# Now let's prepare our training and testing data set with reduced dimension so far.

train <- filter(TimeseriesTransaction, model == "train") %>%
  select(-model)
test <- filter(TimeseriesTransaction, model == "test") %>%
  select(-model)

# select(-model): drops the 'model' column from 
# we have 244 Rows and 14 Columns (after dropping train column) in trian data set. Rest of 60 
```

```{r Linear Model}

#  Creating the linear model using training dataset from above.

retail_lm <- glm(revenue ~ ., data = train) # revenue ~ . = revenue ~ date + index.num + diff + year.iso + wday.xts...

summary(retail_lm)

tidy(retail_lm) %>%
  gather(x, y, estimate:p.value) %>%
  ggplot(aes(x = term, y = y, color = x, fill = x)) +
    facet_wrap(~ x, scales = "free", ncol = 2) +
    geom_bar(stat = "identity", alpha = 0.8) +
    #scale_color_manual(values = palette_light()) +
    #scale_fill_manual(values = palette_light()) +
    theme_tq() + labs(x="", y="")+
    theme(axis.text.x = element_text(angle = 70, vjust = 1, hjust = 1))

# Visualizes the etimate, p.value, statistic and std.error values from summary(retail_lm)
```

```{r Using augment() function, let's add some statistical data, such as fit, residual, standard residual etc.}

augment(retail_lm) %>%
  ggplot(aes(x = date, y = .resid)) +
    geom_hline(yintercept = 0, color = "red") +
    geom_point(alpha = 0.5, color = palette_light()[[1]]) +
    geom_smooth() +
    theme_tq()

# Augment accepts a model object and a dataset and adds information about each observation in the dataset. Most commonly, this includes predicted values in the .fitted column, residuals in the .resid column, and standard errors for the fitted values in a .se.fit column. New columns always begin with a . prefix to avoid overwriting columns in the original dataset.
```
#Visualizing the Prediction test

```{r First let's add prediction and residual linear model to prediction test data. This will help actual prediction. }

# Prepare Preiction test data by passing test data to retail_lm model and resulting adding "pred_lm" and ".resid_lm" colums as output column for add_predictions() and add_residuals() functions. Using dplyr() piping to do both at one go.

pred_test <- test %>%
  add_predictions(retail_lm, "pred_lm") %>%
  add_residuals(retail_lm, ".resid_lm")

# Here we pass the test data to retail_lm model and capture the result to pred_lm variable for each row/revenue value as such that for all the values in row1 revenue value as pred_lm. And we get this result as per the glm retail_lm. Keep in mind though, retail_lm is now augmented using augment() function.
# add_predictions() Add predictions to a data frame. Here pred_lm is the output column added to test dataset. 
# add_residuals() Add Add residuals to a data frame. Here .resid_lm is the output column added to test dataset.
# Finally augmented retail_lm model test data with prediction and residual value is stored in pred_test dataset.

```

```{r Visualize pred_test dataset for residual value with time/date}
pred_test %>%
    ggplot(aes(x = date, y = .resid_lm)) +
    geom_hline(yintercept = 0, color = "red") +
    geom_point(alpha = 0.5, color = palette_light()[[1]]) +
    geom_smooth() +
    theme_tq()
# The Blue line is the predicted test data for retail_lm model and the shaded area represents the rediduals.
```


```{r Visualize pred_test dataset for revenue prediction}
pred_test %>%
  gather(x, y, revenue, pred_lm) %>%
  ggplot(aes(x = date, y = y, color = x)) +
    geom_point(alpha = 0.5) +
    geom_line(alpha = 0.5) +
    # scale_color_manual(values = palette_light()) +
    theme_tq()

# From October 01 to December 01 pred_lm shows the pred_test data in peach (light red) color and actual revenue in blue
```

# Forcasting: We wil start from collecting time index data from DailyTransaction table and use tk_index() function

```{r}
# Extract index ====================================================
TimeIndex <- DailyTransaction %>%
    tk_index()

# tk_index():  Extract an index of date or datetime from time series objects, models, forecasts. tk_index() is used to extract the date or datetime index from various time series objects, models and forecasts. The method can be used on tbl, xts, zoo, zooreg, and ts objects. The method can additionally be used on forecast objects and a number of objects generated by modeling functions such as Arima, ets, and HoltWinters classes to get the index of the underlying data.

# We passed Date from DailyTransaction file to tk_index() function and extracted date and saved it to TimeIndex

TimeseriesTransaction %>%
  ggplot(aes(x = date, y = diff)) +
    geom_point(alpha = 0.5, aes(color = as.factor(diff))) +
    geom_line(alpha = 0.5) +
    #scale_color_manual(values = palette_light()) +
    theme_tq()
# taking date and diff into consideration, plotting diff (in y) against date(in x). Here we observe some gaps in the dates. Most common gaps are coming from weekends. And rest are coming from possibly holidays.
dim(TimeseriesTransaction)
# 304 Rows 15 Columns
View(TimeseriesTransaction)
```
#=======================

```{r }
TimeseriesTransaction %>%
  select(date, wday.lbl, diff) %>%
  filter(wday.lbl != "Sunday" & diff > 86400) %>%
  mutate(days_missing = diff / 86400 -1)

# Adding number of missing days as a column. This will give us an idea for no. of days as holiday and identify the dates.
```

```{r Holidays }
DaysOff <- c("2010-12-24", "2010-12-25", "2010-12-26", "2010-12-27", "2010-12-28", "2010-12-29", "2010-12-30", "2010-01-01", "2010-01-02", "2010-01-03", "2011-04-22", "2011-04-23", "2011-04-24", "2011-04-25", "2011-05-02", "2011-05-30", "2011-08-29", "2011-04-29", "2011-04-30") %>% 
ymd()
```

```{r}
FutureIndex <- TimeIndex %>%
    tk_make_future_timeseries(n_future = 300, inspect_weekdays = TRUE, inspect_months = FALSE, skip_values = DaysOff)
FutureIndex %>%
    tk_get_timeseries_signature() %>%
    #tk_get_timeseries_summary() %>%
    ggplot(aes(x = index, y = diff)) +
    geom_point(alpha = 0.5, aes(color = as.factor(diff))) +
    geom_line(alpha = 0.5) +
    #scale_color_manual(values = palette_light()) +
    theme_tq()

# tk_make_future_timeseries(): Make future time series from existing. n_future: No. of days prediction is needed,
# inspect_weekdays: Uses a logistic regression algorithm to inspect whether certain weekdays (e.g. weekends) should be excluded from the future dates. Default is FALSE.
# inspect_months: Uses a logistic regression algorithm to inspect whether certain days of months (e.g. last two weeks of year or seasonal days) should be excluded from the future dates. Default is FALSE.
# skip_values: A vector of same class as idx of timeseries values to skip. We are skipping Sunday and Holidays here.
# tk_get_timeseries_signature(): Get date features from a time-series index. See below for more info.
```

#====================

```{r }
FutureData <- FutureIndex %>%
    tk_get_timeseries_signature() %>%
    rename(date = index)

# tk_get_timeseries_signature():tk_get_timeseries_signature decomposes the timeseries into commonly needed features such as numeric value, differences, year, month, day, day of week, day of month, day of year, hour, minute, second.

prediction <- predict(retail_lm, newdata = FutureData)

# using predict() function predicting future data by passing retail_lm and FutureData timeseries as parameter.

prediction <- FutureData %>%
    select(date) %>%
    add_column(revenue = prediction)

# Adding revenue as prediction from above along with date. This will help forsee the future revenue data.

DailyTransaction %>% 
  select(SalesDate, TotalRevenueFromSales) %>% # selecting SalesDate, TotalReveueFromSales from DailyTransaction
  rename(date = SalesDate, revenue = TotalRevenueFromSales) %>% # renaming SalesDate to date and revenue revenue
  rbind(prediction) %>% # adding prediction data using rbind() function.
  ggplot(aes(x = date, y = revenue)) +
    scale_x_date() +
    geom_vline(xintercept = as.numeric(max(DailyTransaction$SalesDate)), color = "red", size = 1) +
    geom_point(alpha = 0.5) +
    geom_line(alpha = 0.5) +
    geom_point(aes(x = date, y = revenue), data = prediction,
               alpha = 0.5, color = palette_light()[[3]]) +
    geom_smooth(aes(x = date, y = revenue), data = prediction,
                method = 'loess', color = "cyan") +
    theme_tq()

# Displaying future revenue data after the red line.
```
# Prediction Completed. let's do the Residual Test.

```{r All Action relted to ".resid_lm" value from pred_test data}

ResidualTest <- pred_test$.resid_lm

SDResidualTest <- sd(ResidualTest, na.rm = TRUE)

prediction <- prediction %>%
    mutate(
        lo.95 = revenue - 1.96 * SDResidualTest, #95% Confidence Interval
        lo.80 = revenue - 1.28 * SDResidualTest, #80% Confidence Interval
        hi.80 = revenue + 1.28 * SDResidualTest,
        hi.95 = revenue + 1.96 * SDResidualTest
        )
```

```{r } 
DailyTransaction %>%
  select(SalesDate, TotalRevenueFromSales) %>%
  rename(date = SalesDate, revenue = TotalRevenueFromSales) %>%
  ggplot(aes(x = date, y = revenue)) +
    geom_point(alpha = 0.5) +
    geom_line(alpha = 0.5) +
    geom_ribbon(aes(ymin = lo.95, ymax = hi.95), data = prediction, 
                fill = "#D5DBFF", color = NA, size = 0) +
    geom_ribbon(aes(ymin = lo.80, ymax = hi.80, fill = key), data = prediction,
                fill = "#596DD5", color = NA, size = 0, alpha = 0.8) +
    geom_point(aes(x = date, y = revenue), data = prediction,
               alpha = 0.5, color = palette_light()[[2]]) +
    geom_smooth(aes(x = date, y = revenue), data = prediction,
                method = 'loess', color = "#98FB98") +
    theme_tq()
```

# Displaying prediction for 257 days starting from last transaction date of DailyTransaction (09December2011). The prediction is upto 2012-10-04
