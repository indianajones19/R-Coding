---
title: "Retail_Data_Forcasting"
author: "Muhammad Rahman"
date: "April 5, 2020"
output: rmd file
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## R Markdown

## Initial Data Analysis:

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)

setwd("H:/Ryerson Data Science and Analytics/06. CKME 136 - Capstone Project/MSR Capstone Project/DataSet from UCI and Code Capstone")

RawRetailData <- read.csv(file= "Online_Retail.csv", header = TRUE, sep = ",", dec = ",", fill = TRUE, comment.char = "")

View (RawRetailData)

str (RawRetailData)

sum(is.na(RawRetailData)) ## Checking NA. 135080 found

colSums(is.na(RawRetailData)) ## Checking which column has how many NA. All NA found in CustomerID.

sapply(RawRetailData, function(x) length(unique(x))) ## Checking unique number of items of each column.

## Note: we have 135,080 missing value in CustomerID leaving us with 406,829 entries with CustomerID. There are 4373 unique Customers among 406,829 CustomerIDs. We assume these missing values represents a number of guest shoppers unless there are really missing informaiton here. However, even if we do not consider CustomerID, our analysis can be done leveraging rest of the data points. Let's move on.

## We have 38 unique Country. Decided to keep this value as factor.

## We need to know sales informaiton. Therefore, Quantitiy and UniPrice will be two important data. We will convert both of them to numeric, Quantity as Integer and UnitPrice as Float. Uniqe value is not considerable here.

## There is StockCode and Description of the item representing products sold. we will consider only StockCode.

## Further analysis revealed, IvoiceNo will be the best to consider in terms of identifing sales since each sale must have an InvoiceNo and there is no missing data. 

## Now that we are good with InvoiceNo to identify each sale and determine actual monetary value of each sale by multiplying Quantity with UnitPrice and cll it Sales. We are good to go with analysis of time series data. By the way, please consider there are 25,900 unique InvoiceNo indicating customer purchasing from 38 different counteis. 

## We will take InvoiceDate and split it into SalesDate and SalesTime. Furhter we will extract DayOfWeek, Month, and Year. Finally we will determine SellStatus by analysing Sales value. Positive values indicate Sold and Negetive valuse indicating Returned. 

```


```{r loading required packages}

#========================================= Initial Setup ==============================================

#create a function to check for installed packages and install them if they are not installed

install <- function(packages){
  new.packages <- packages[!(packages %in% installed.packages()[, "Package"])]
  if (length(new.packages)) 
    install.packages(new.packages, dependencies = TRUE)
  sapply(packages, require, character.only = TRUE)
} 

# Required packages:

required.packages <- c("caret", "broom", "ggcorrplot","matrixStats", "tidyverse", "timetk", "prophet", "tidyquant", "modelr", "gridExtra", "grid")

install(required.packages)

options(na.action = na.warn)

install.packages("tidyverse")
library(tidyverse)

install.packages("caret")
library(caret)

install.packages("broom")
library(broom)

install.packages("ggplot2")
library(ggplot2)




```

Package Description:
caret: The caret package (short for Classification And REgression Training) is a set of functions that attempt to streamline the process for creating predictive models. The package contains tools for: data splitting. pre-processing
broom: broom is a package for tidying statistical models into data frames
ggcorrplot: This package is used to visualize easily a correlation matrix using 'ggplot2'. It provides a solution for reordering the correlation matrix and displays the significance level on the plot. It also includes a function for computing a matrix of correlation p-values.
matrixStats: Functions that Apply to Rows and Columns of Matrices (and to Vectors). matrixStats is a high-performing functions operating on rows and columns of matrices, e.g. col / rowMedians(), col / rowRanks(), and col / rowSds(). Functions optimized per data type and for subsetted calculations such that both memory usage and processing time is minimized. There are also optimized vector-based methods, e.g. binMeans(), madDiff() and weightedMedian().
tidyverse: The tidyverse is an opinionated collection of R packages designed for data science. All packages share an underlying design philosophy, grammar, and data structures. See how the tidyverse makes data science faster, easier and more fun with “R for Data Science”
timetk: A Tool Kit for Working with Time Series in R. timetk is used for easy visualization, wrangling, and preprocessing of time series data for forecasting and machine learning prediction. 
prophet: Prophet is a procedure for forecasting time series data based on an additive model where non-linear trends are fit with yearly, weekly, and daily seasonality, plus holiday effects.
tidyquant: Tidy Quantitative Financial Analysis: This R package brings business and financial analysis to the 'tidyverse'. The 'tidyquant' package provides a convenient wrapper to various 'xts', 'zoo', 'quantmod', 'TTR' and 'PerformanceAnalytics' package functions and returns the objects in the tidy 'tibble' format. The main advantage is being able to use quantitative functions with the 'tidyverse' functions including 'purrr', 'dplyr', 'tidyr', 'ggplot2', 'lubridate', etc. 
modelr:  Modeling Functions that Work with the Pipe. This is a set of functions for modeling that helps seamlessly integrate modeling into a pipeline of data manipulation and visualization. This set of functions imports ‘broom’, ‘magrittr’, ‘purrr’, ‘rlang’, ‘tibble’, ‘tidyr’, ‘tidyselect’, ‘vctrs’. 
gridExtra: Miscellaneous Functions for "Grid" Graphics
Provides a number of user-level functions to work with "grid" graphics, notably to arrange multiple grid-based plots on a page, and draw tables.
grid: An R function to add grid to graphs.
prophet: Prophet is a procedure for forecasting time series data based on an additive model where non-linear trends are fit with yearly, weekly, and daily seasonality, plus holiday effects. ... Prophet is open source software released by Facebook's Core Data Science team. It is available for download on CRAN and PyPI.
predict: The prediction and margins packages are a combined effort to port the functionality of Stata's (closed source) margins command to (open source) R. prediction is focused on one function - prediction() - that provides type-safe methods for generating predictions from fitted regression models.

## Dataset Loading and Preprocessing
## Here is the code used to read actual data and transform it as planned.

```{r load retail dataset, echo=FALSE}
RetailData <- read_csv("Online_Retail.csv",
                   col_types = cols(
                      InvoiceNo = col_character(),
                      StockCode = col_character(),
                      Description = col_character(),
                      Quantity = col_integer(),
                      InvoiceDate = col_datetime("%m/%d/%Y %H:%M"),
                      UnitPrice = col_double(),
                      CustomerID = col_integer(),
                      Country = col_character()
                      )) %>%

mutate(SalesDate = parse_date(format(InvoiceDate, "%Y-%m-%d")),
         WeekDay = wday(SalesDate, label = TRUE),
         SalesTime = parse_time(format(InvoiceDate, "%H:%M")),
         Month = format(InvoiceDate, "%m"),
         Sales = Quantity * UnitPrice,
         SoldReturned = ifelse(Quantity > 0, "Sold", "Returned"))

dim(RetailData)

```

# Data oraganization


```{r}
View(RetailData)

write.csv(RetailData, "NewRetailData.csv")

str(RetailData)
```

MSR Comments:
Initial analysis reveals:
Total No or Rows: 541909
There is no NA value in any columnn except in CustomerId
Total NA value in CustomerId = 135080
This leaves 541909 - 135080 = 406829 Rows of data.

We could not find any logic to impute the CustomerId value in the dataset. There are transaction data for other datafields where CustomerId is missing. this could be the result of guest purchases.
We can assume a number for guest and impute CustomerId field - Ask Professor.
Otherwise we have 406829 rows of data for our analysis.

Now it is time to get missing and unique values in each columns considered for data analysis.

```{r checking missing value }
colSums(is.na(RetailData))
```

# Above code result shows that, there are 1454 missing value in Description and 135080 in CustomerID column. We discussed about CustomerID before. As we see StockCode is correspondent for Description and StockCode does not have any missing value, we will consider StockCode in our Analysis and ignore Description.


#Exploratory Data Analysis


```{r Unique items in Column 1, 2, 7 & 8, meaning column: InvoiceNo, StockCode, CustomerId and Country }
sapply(RetailData[,c(1, 2, 7, 8, 9)], function(x)length(unique(x)))
```

We have 25900 unique InvoiceNo,  4070 unique StockCode, 4373 unique CustomerID, 38 unique Country and 305 unique SalesDate in the dataset out of a total of 541909 records.

Therefore, we can conclude:

There are 25900 orders placed where there were multiple items in the order. These items are one or more representing the 4070 StockCodes.

4373 Uniques customers placed these orders from 38 different countries. means, many of these customers are repeatign customers placed variety of orders combination of multiple items in each order.

Obviously, these repeating customers placed their orders in different time of the year.

305 unique SalesDate tells us on an average, on a given day 541909/305 = 1777 transactions happened in the store. If we can somehow consolidate and/or analyze data for each day and perform our Data Analysis, this will give us opportunity to work on a smaller dataset with greater analytics and forecasting capability. However, we have to get a clear picture for each days' transactional data.


```{r Let's start with Country. Name of countries:  }
unique(RetailData$Country)

# Since this data is a UK based online retail store, possibility is there that maximum sales may come from UK. Let's check it out.

sum(RetailData$Country == "United Kingdom")

# 495478 records out of 541909 rows found for United Kingdom
```

# Let's see how many of each item sold. We will see first 6 records as sample.
```{r Number of items sold}
RetailData %>%
  group_by(StockCode, Description) %>%
  summarise(TotalQuantity = sum(Quantity)) %>%
  arrange(-TotalQuantity) %>% head()
```

# Let us plot country wide selling trend. We will keep United Kingdom separate from the rest.

```{r Courntry wise selling trend. Plot1 is only for United Kingdom and Plot2 is for rest }

UKSales <- RetailData %>%
  filter(Country == "United Kingdom") %>%
ggplot(aes(x = Country, fill = SoldReturned)) + geom_bar(alpha = 0.8) + scale_fill_grey(start = 0.8, end = 0.2, na.value = 'red', aesthetics = "fill") + theme_tq_green() 

OtherSales <- RetailData %>%
  filter(Country != "United Kingdom") %>%
  ggplot(aes(x = Country, fill = SoldReturned)) + geom_bar(alpha = 0.8) + scale_fill_grey(start = 0.8, end = 0.2, na.value = 'red', aesthetics = "fill") + theme_tq_green() + theme(axis.text.x = element_text(angle = 90, vjust = 1, hjust = 1)) + labs(y = "No. of Items Sold/Returned")

title <- textGrob("UK and Other Countries - Sales trend", gp = gpar(fontface = "bold", cex = 1.5))

grid.arrange(UKSales, OtherSales, top=title, widths = c(0.2, 1.2))

```


```{r SoldReturned, Sold / Returned Analysis over time}

RetailData %>%
ggplot(aes(x = SalesDate, color = SoldReturned)) + ggtitle("Sell/Returns over time") + 
    theme(plot.title = element_text(hjust = 0.5))+ 
    facet_grid(SoldReturned ~., scales = "free") + geom_freqpoly(bins = 90, size = 1, alpha = 1) +
    scale_color_tq(theme = "green") + labs(y = "Sell/Return Count")
```

# Purchases over the time of the day
```{r Sales over time during the day, compared over time}
RetailData %>%
ggplot(aes(x = SalesTime, y = SalesDate)) + ggtitle("Trend of Transaction over time in a day") +   theme(plot.title = element_text(hjust = 0.5))+
stat_bin_2d(alpha = 1, bins = 20, color = "white") +
scale_fill_gradientn(colours = c(palette_light()[[1]], palette_light()[[2]])) 

```

# Sales per day over the months of the year
```{r Sales per day over Month}
RetailData %>%
  mutate(day = format(InvoiceDate, "%d")) %>%
  group_by(Month, day) %>%
  summarise(Sales = sum(Sales)) %>%
  ggplot(aes(x = Month, y = day, fill = Sales)) + ggtitle("Each Day Sales Over Month") +   theme(plot.title = element_text(hjust = 0.5))+ theme(legend.position = "right") + ylab("Days of the Month")+ xlab("Months") + geom_tile(alpha = 1, color = "white") + scale_fill_gradientn(colours = c(palette_light()[[1]], palette_light()[[2]]))

```
# From January to December, the graph above shows sales of each day of the Month. Overall sales grew uniformly throuout the year and took a spike twoards the end the year. The final quarter was good.
 
# Let's revisit quantities of item sold.
```{r Number of items sold}
RetailData %>%
  group_by(StockCode, Description) %>%
  summarise(TotalQuantity = sum(Quantity)) %>%
  arrange(-TotalQuantity) %>% head()

```
# As we see from above code result, different items (as per StockCode) is sold in different quantity. We like to see them in groups. Such as sold below 10,000, in between 10,000 to 50,000, more than 50,000.

# 
```{r Sum of all Quantity }

# sum of Quantitiy including returns

p1 <- RetailData %>%
  group_by(StockCode, Description) %>%
  summarise(TotalQuantity = sum(Quantity)) %>%
  ggplot(aes(x = TotalQuantity)) + 
              geom_density(fill = palette_light()[[1]], alpha = 1) + theme_tq() + xlab("Items with Returns") + ylab(" ")

# Sum of Quantitiy without returns 

p2 <- RetailData %>%
  group_by(StockCode, Description) %>%
  summarise(TotalQuantity = sum(Quantity)) %>%
  filter(TotalQuantity > 1) %>%
  ggplot(aes(x = TotalQuantity)) + 
              geom_density(fill = palette_light()[[1]], alpha = 1) + theme_tq() + xlab("Items Sold") + ylab(" ")

# Sum of Quantitiy with high volume ( > 10000)

p3 <- RetailData %>%
  group_by(StockCode, Description) %>%
  summarise(TotalQuantity = sum(Quantity)) %>%
  filter(TotalQuantity > 10000) %>%
  ggplot(aes(x = TotalQuantity)) + 
              geom_density(fill = palette_light()[[1]], alpha = 1) + theme_tq() + xlab("Items Sold with High Volume") + ylab(" ")

title1 <- textGrob("Quantity wise product sell trend", gp = gpar(fontface = "bold", cex = 1.5))    
grid.arrange(p1, p2, p3, ncol = 3,top=title1 ,widths = c(0.6, 0.5, 0.7))

```

# For Business, it is important to know What are the best items in store. Such as items sold in highest quantity at fastest pace versus items sold at lowest quantity or may be slowest moving itmes. For our analysis we will only find to Sales and Transaction and discurd the low volume slow moving items.

```{r}
TopSellingItems <- RetailData %>%
  group_by(SalesDate, StockCode, Description) %>%
  summarise(sum = sum(Quantity)) %>%
  group_by(StockCode, Description)%>%
  summarise(n = n()) %>%
  arrange(-n)

TopTransactionItems <- RetailData %>%
  filter(StockCode %in% TopSellingItems$StockCode[1:5]) %>%
  group_by(SalesDate, StockCode) %>%
  summarise(sum = sum(Quantity)) %>%
  spread(key = StockCode, value = sum)

RetailData %>%
  filter(StockCode %in% TopSellingItems$StockCode[1:5]) %>%
  group_by(SalesDate, StockCode, Description) %>%
  summarise(sum = sum(Quantity)) %>%
    ggplot(aes(x = SalesDate, y = sum)) +
    facet_wrap(~ StockCode, ncol = 1, scales = "free") +
    geom_line(color = palette_dark()[[6]], size = 1, alpha = 0.8) +
    theme_tq() +
    labs(Title = "Five Topmost Items Sold by Quantity", x = "Five Topmost Items Selling Trend", 
         y = "Sold Item Quantity")

```

# The most sold item is "WHITE HANGING HEART T-LIGHT HOLDER" with StockCode 85123A. It is sold 304 times. Which means almost every day this item was sold from the store. Let's analyze the sales trend of only this item:

```{r }
RetailData %>%
  filter(StockCode == "85123A") %>%
  group_by(SalesDate, SoldReturned) %>%
  summarise(HighestSellingItem = sum(Quantity)) %>%
  ggplot(aes(x = SalesDate, y = HighestSellingItem, color = SoldReturned)) + ggtitle("Highest selling item's transaction record") +   theme(plot.title = element_text(hjust = 0.5))+ ylab("Quantities")+
    facet_wrap(~ SoldReturned, ncol = 1, scales = "free") +
    geom_line(size = .4, alpha = 1) + 
    scale_color_tq(values = palette_dark()) 
    
```


# For any business, the central point of success is customer. From business point of view, an entreprenure must take utmost care of his/her loyal customers. Let's see how many recurring customer are there. To start with, we will analyze frequency of transactions done by customers on our sales data.

```{r Recurring/Repeating and One Time Customers}

TransactionsFrequency <- RetailData %>%
  group_by(SalesDate, CustomerID) %>%
  summarise(sum = sum(Quantity)) %>%
  group_by(CustomerID) %>%
  summarise(n = n()) %>%
  mutate(CustomerType = ifelse(n > 1, "RepeatingCustomers", "OneTimeCustomer"))

TransactionsFrequency 

# Shows a tabl of CustomerID & No of transaction (n) at first. n = 1 is One Time and n > 1 Repeating Customer. 

length(which(TransactionsFrequency$CustomerType == "RepeatingCustomers")) 

# There are 2992 repeating customers in the store.
```

# Remember, we need to concentrate on datewise/daily analysis. At this point, we need to see how many one time and how many repeating customers visited the store on a given day and we will analyze further from there.

```{r Repeating and One Time Customers }

CustomerFrequency <- left_join(RetailData, TransactionsFrequency, by = "CustomerID") %>%
  distinct(SalesDate, CustomerID, CustomerType) %>%
  group_by(SalesDate, CustomerType) %>%
  summarise(n = n()) %>%
  spread(key = CustomerType, value = n)

CustomerFrequency

TransactionsFrequency %>%
  group_by(CustomerType) %>%
  summarise(n = n()) %>%
  mutate(Ratio = n / sum(n)) %>%
  ggplot(aes(x = "", y = Ratio, fill = CustomerType)) +ggtitle("One time and repeating customer ratio") +   theme(plot.title = element_text(hjust = 0.5))+ ylab("")+ xlab("")+
    geom_bar(stat = "identity", alpha = 1) +
    coord_polar("y", start = 0) +
    scale_fill_tq(values = palette_dark()) 
    
```

# We need to know daywise how many items sold, their mean, what is daily sales total and mean sales.

```{r Mean items sold, mean quantitity sold per customer and mean Sales per customer on daily basis}

purchases <- RetailData %>%
  group_by(SalesDate, CustomerID) %>%
  summarise(n = n(),
            TotalQuantity = sum(Quantity),
            TotalSales = sum(Sales)) %>%
  group_by(SalesDate) %>% 
# Upto this much, what we get is, on a give day (say, 01Dec2010), a customer(12431) purchased 14 different items(as per 14 different StockCode) whose total Quantity is 107 (e.g. he purchased 6 of StockCode 22941, 8 of StockCode 21622 etc.). This is TotalQuantity = 107. TotalSales is the sum of Sales for those 107 Quantites/items purchased (in this case it is 358).
  summarise(MeanSalesPerCustomer = mean(TotalSales),
            MeanQuantityPerCustomer = mean(TotalQuantity),
            MeanOrdersPerCustomer = mean(n))
purchases
# Now that in summary, using summarise() function, we agregrated these values as explained below. Please note that, all data at this point is grouped by SalesDate. Meaning, each of the following variables represetn data for one day of the year and this will be our input to datamodel file.

# MeanSalesPerCustomer: (sum/total Sales made to each customer on a given day) / (no of unique customer)
# MeanQuantityPerCustomer: (sum/total Quantity/items sold to each customer on a given day) / (no of unique customer)
# MeanOrdersPerCustomer: (sum/total number of Quantity sold to each customer[total number of order placed by customer on a given day] on a given day) / (no of unique customer)

# Visualize:

purchases %>%
  gather(x, y, MeanSalesPerCustomer:MeanOrdersPerCustomer) %>%
  ggplot(aes(x = SalesDate, y = y)) + ggtitle("Sales trend by each customer") + theme(plot.title = element_text(hjust = 0.5))+ ylab("")+ xlab("")+
    facet_wrap(~ x, ncol = 1, scales = "free") +
    geom_line(color = palette_light()[[1]], size = 1, alpha = 1) +
    geom_smooth(color = palette_light()[[2]], method = 'loess') 
    
```

#  Let's analyze the Sold and Returned itme patterns

```{r Sold Returned items analysis (on daily basis grouped by SalesDate)}

ItemsSoldReturned <- RetailData %>%
  group_by(SalesDate, SoldReturned) %>%
  summarise(TotalQuantity = sum(Quantity)) %>%
  spread(key = SoldReturned, value = TotalQuantity)

ItemsSoldReturned

ItemsSoldReturned%>%
  gather(x, y, Sold:Returned) %>%
  ggplot(aes(x = SalesDate, y = y, color = x)) + 
    geom_line(size = 1, alpha = 0.8) +
    theme_tq() +
    labs(x = "", 
         y = "Items Quantity",
         title = "Items Sold and Returned over time")

```

# How many different items are purchased/returned per day?

```{r transactions per day: purchased and returned both}

TransactedItems <- RetailData %>%
  group_by(SalesDate, StockCode) %>%
  summarise(n = n()) %>%
  group_by(SalesDate) %>%
  summarise(TransactedItems = n())

TransactedItems

# This is telling us how many unique items (as per StockCode) were transacted (Sold and/or Returned)

TransactedItems %>%
  ggplot(aes(x = SalesDate, y = TransactedItems)) +
    geom_line(color = palette_light()[[6]], size = 1, alpha = 0.8) +
    geom_smooth(color = palette_light()[[2]], method = 'loess') +
    theme_tq() +
    labs(title= "Items Transacted (Sold and/or Returned) Over the year", x = "Quarters/Seasons", 
         y = "Number of Items",
         color = "")
```

```{r Calculate Net Sales: Total, mean Sales per day and total, mean Quantity sold/returned per day}

NetSales <- RetailData %>%
  group_by(SalesDate) %>%
  summarise(TotalSalesPerDay = sum(Sales),
            MeanSalesPerDay = mean(Sales), 
            TotalQuantitySoldPerDay = sum(Quantity),
            MeanQuantitySoldPerDay = mean(Quantity))

NetSales

# NetSales: Total Sales (Income) from both Sold and Returned items per day
# TotalSalesPerDay: Summation of Sales on a given day
# MeanSalesPerDay: Summation of Sales on a given day / Total items sold andor returned (Total itmes of StockCode wise totals on a given day)
# TotalQuantitySoldPerDay: Summation of Quantity sold andor returned on a given day 
# MeanQuantitySoldPerDay: Summation of Quantity sold andor returned on a given day / Total items sold andor returned (Total itmes of StockCode wise totals on a given day)

NetSales %>%
  gather(x, y, TotalSalesPerDay : MeanQuantitySoldPerDay) %>%
  ggplot(aes(x = SalesDate, y = y)) +
    facet_wrap(~ x, ncol = 1, scales = "free") +
    geom_line(color = palette_light()[[1]], size = 1, alpha = 0.8) +
    geom_smooth(color = palette_light()[[2]], method = 'loess') +
    theme_tq() +
    labs(title="Mean and Sum of Sales and Quantity", x = "Quarters/Seasons", 
         y = "Number / Volume of Items")
```


# Revenue from Sales and Quantity analysis (Only sold items to consider and analyze)
```{r }
Revenue <- RetailData %>% # Revenue = sum of sales of items
  filter(Sales > 0) %>%
  group_by(SalesDate) %>%
  summarise(TotalRevenueFromSales = sum(Sales), 
            MeanRevenueFromSales = mean(Sales),
            QuantitySoldForRevenue = sum(Quantity),
            MeanQuantitySoldForRevenue = mean(Quantity))

# IMPORTANT: DONT CHANGE "TotalRevenueFromSales" TO 'revenue' here. It is changed in "TimeseriesTransaction". Line: 615

Revenue

Revenue %>%
  gather(x, y, TotalRevenueFromSales:MeanQuantitySoldForRevenue) %>%
  ggplot(aes(x = SalesDate, y = y)) +
    facet_wrap(~ x, ncol = 1, scales = "free") +
    geom_line(color = palette_light()[[1]], size = 1, alpha = 0.8) +
    geom_smooth(color = palette_light()[[2]], method = 'loess') +
    theme_tq() +
    labs(title= "Revenue from Sales and Quantity sold over the year", x = "Quarters/Seasons", 
         y = "Number / Volume of Items")
```


# Sales analysis on Returned Items (Sales Returned - Refund paid back to customer) including Quantity Returned analysis
```{r}
SalesReturned <- RetailData %>% # Sales Returned - Refund paid back to customer
  filter(Sales < 0) %>%
  group_by(SalesDate) %>%
  summarise(TotalSalesReturned = sum(Sales),
            MeanSalesReturned = mean(Sales),
            TotalQuantityReturned = sum(Quantity),
            MeanQuantityReturned = mean(Quantity))

SalesReturned

SalesReturned %>%
  gather(x, y, TotalSalesReturned:MeanQuantityReturned) %>%
  ggplot(aes(x = SalesDate, y = y)) +
    facet_wrap(~ x, ncol = 1, scales = "free") +
    geom_line(color = palette_light()[[2]], size = 1, alpha = 0.8) +
    theme_tq() +
    labs(title= "SalesReturned from Sales and Quantity returned over the year", x = "Quarters/Seasons", y = "Number / Volume of Items")
```


```{r Arrange and prepare Data for modeling}

# We have UnitPrice. We need to know "Unit-Quantity". Our target is to find out what are the items were sold (as per StockCode) and how many of them were sold each day. Once we get this, we can get mean of each items sold  By Unit-Quantity, we mean: 


UnitQuantity <- distinct(select(RetailData, SalesDate, StockCode, UnitPrice)) %>% # selecting distinct SalesDate, StockCode and UnitPrice.
  mutate(UnitQuantity = paste(SalesDate, StockCode, sep = "_")) %>% # using mutate() pasting SalesDate and StockCode with underscore and calling it UnitQuantity, 
  select(UnitQuantity, UnitPrice) # projecting UnitQuantity with UnitPrice. This will help us calcualting mean UnitPrice against StockCode.

UnitQuantity <- distinct(select(RetailData, SalesDate, StockCode, UnitPrice)) %>% 
  mutate(UnitQuantity = paste(SalesDate, StockCode, sep = "_")) %>% 
  select(UnitQuantity, UnitPrice)


MeanPricePerUnit <- RetailData %>%
  filter(SoldReturned == "Sold") %>%
  group_by(SalesDate, StockCode) %>%
  summarise(n = n()) %>%
  mutate(UnitQuantity = paste(SalesDate, StockCode, sep = "_")) %>%
  left_join(UnitQuantity, by = "UnitQuantity") %>% # adding UnitQuantity table
  group_by(SalesDate, StockCode) %>%
  summarise(mean = mean(UnitPrice)) %>%
  group_by(SalesDate) %>%
  summarise(MeanPricePerUnit = mean(mean))

MeanPricePerUnit %>%
     ggplot(aes(x = SalesDate, y = MeanPricePerUnit)) +
     geom_line(color = palette_light()[[1]], size = 0.8, alpha = 1) +
     theme_tq_green() +
     labs(title = "MeanPricePerUnit changes over the year", x = "Quarters/Seasons", 
          y = "Mean Price Per Unit")

```


# Forcasting
```{r Putting it all together for forcasting data preparation}

DailyTransaction <- distinct(select(RetailData, SalesDate, WeekDay, Month)) %>%
  left_join(NetSales, by = "SalesDate") %>%
  left_join(MeanPricePerUnit, by = "SalesDate") %>%
  left_join(Revenue, by = "SalesDate") %>%
  left_join(SalesReturned, by = "SalesDate") %>%
  left_join(purchases, by = "SalesDate") %>%
  left_join(CustomerFrequency, by = "SalesDate") %>%
  left_join(ItemsSoldReturned, by = "SalesDate") %>%
  left_join(TransactedItems, by = "SalesDate") %>%
  left_join(TopTransactionItems, by = "SalesDate") %>%
  mutate(DifferenceInTotalRevenue = TotalRevenueFromSales - lag(TotalRevenueFromSales),
         Season = ifelse(Month %in% c("03", "04", "05"), "Spring",
                         ifelse(Month %in% c("06", "07", "08"), "Summer",
                                ifelse(Month %in% c("09", "10", "11"), "Fall", "Winter"))))


colnames(DailyTransaction)[grep("^[0-9]+", colnames(DailyTransaction))] <- paste0("P_", colnames(DailyTransaction)[grep("^[0-9]+", colnames(DailyTransaction))])

dim(DailyTransaction)
# 305 Rows 31 Columns. Row 1 has one NA value. We will work on Dimentionality reduction.
```

```{r Split Data into train and test dataset. SalesDate before Oct2011}
DailyTransaction <- DailyTransaction %>%
  mutate(model = ifelse(SalesDate <= "2011-10-01", "train", "test"))

```
=================== Submitted up to this much as Initial Code Submission ==============================

```{r Visualization Train and Test data}

DailyTransaction %>%
  ggplot(aes(x = SalesDate, y = TotalRevenueFromSales, color = model)) +
    geom_point(alpha = 0.5) +
    geom_line(alpha = 0.5) +
    #labs(title = "Training/Test Data Segregation", x = "Quarters", y = "Revenue")
    theme_tq()
```

```{r Creating Time Series Data for revenue. We can compare many other columns as such. At this point we are considering Revenue as primary attribute to generate timeseries data and analyse it.}

TimeseriesTransaction <- DailyTransaction %>%
  rename(date = SalesDate) %>%
  rename(revenue = TotalRevenueFromSales) %>%
  select(model, date, revenue) %>% 
  tk_augment_timeseries_signature() %>%
  select(-contains("Month"))


# tk_augment_timeseries_signature(.data, .date_var = NULL): Add many time series features to the data. 
# .data: A time-based tibble or time-series object.
# .date_var: For tibbles, a column containing either date or date-time values. If NULL, the time-based column will interpret from the object (tibble, xts, zoo, etc)
# tk_augment_timeseries_signature() adds 25+ time series features including:
# Trend in Seconds Granularity: index.num
# Yearly Seasonality: Year, Month, Quarter
# Weekly Seasonality: Week of Month, Day of Month, Day of Week, and more
# Daily Seasonality: Hour, Minute, Second
# Weekly Cyclic Patterns: 2 weeks, 3 weeks, 4 weeks

dim(TimeseriesTransaction)
# 305 28

# At this point our target is to reduce dimension as much as possible and only keep necessary data for our analysis.

# First step: Remove NA value:

TimeseriesTransaction <- TimeseriesTransaction[complete.cases(TimeseriesTransaction), ]

# complete.cases(): Return a logical vector indicating which cases are complete, i.e., have no missing (NA) values 

dim(TimeseriesTransaction)
# 304 28, Top row deleted due to NA value in diff column. diff = index.number - lag(index.number)
```

```{r Now we will target finding zero Variance in data and remove them.}

(var <- data.frame(
          colnames = colnames(TimeseriesTransaction[, sapply(TimeseriesTransaction, is.numeric)]),
          colvars = colVars(as.matrix(TimeseriesTransaction[, sapply(TimeseriesTransaction, is.numeric)]))) %>%
  filter(colvars == 0))

# colVars(): Variance estimates for each row (column) in a matrix.
# Identifying the columns having single value resulting no Variance. i.e. Variance = 0. We found 5 such columns. Namely, hour, minute, second, hour12, am.pm. These columns will not add any value to our analysis. Discarding 
```

```{r}
TimeseriesTransaction <- select(TimeseriesTransaction, -one_of(as.character(var$colnames)))

```

# Now we will detect correlation amount variables and remove highly correlated features

```{r To get Correlation, we need only numeric values from dataset}

# First using sapply() creating vector TimeseriesTransaction and then finding correlation among them. Storing in variable 'relation'

relation <- cor(TimeseriesTransaction[, sapply(TimeseriesTransaction, is.numeric)])

# let's compute p-value for correlation:

p.relation <- cor_pmat(TimeseriesTransaction[, sapply(TimeseriesTransaction, is.numeric)])

# cor(): Compute correlation between numeric variables.
# cor_pmat(): Compute a correlation matrix p-values.

ggcorrplot(relation, p.mat = p.relation, hc.order = TRUE, type = "lower", outline.col = "white",  
           colors = c(palette_light()[1], "grey", palette_light()[2]))

# Explaination: ... 

# ggcorrplot(relation,  type = "upper", outline.col = "white", hc.order = TRUE, p.mat = p.relation,
           colors = c(palette_light()[1], "white", palette_light()[2]))
```

```{r Finding Highly Correlated values}

correlation <- findCorrelation(relation, cutoff=0.8) 
TimeseriesTransaction <- select(TimeseriesTransaction, -one_of(colnames(relation)[correlation]))

dim(TimeseriesTransaction)
# 304 Rows 15 Columns. 

# Now let's prepare our training and testing data set with reduced dimension so far.

train <- filter(TimeseriesTransaction, model == "train") %>%
  select(-model)
test <- filter(TimeseriesTransaction, model == "test") %>%
  select(-model)

# select(-model): drops the 'model' column from 
# we have 244 Rows and 14 Columns (after dropping train column) in trian data set. Rest of 60 
```

```{r Linear Model}

fit_lm <- glm(revenue ~ ., data = train)

summary(fit_lm)

tidy(fit_lm) %>%
  gather(x, y, estimate:p.value) %>%
  ggplot(aes(x = term, y = y, color = x, fill = x)) +
    facet_wrap(~ x, scales = "free", ncol = 2) +
    geom_bar(stat = "identity", alpha = 0.8) +
    #scale_color_manual(values = palette_light()) +
    #scale_fill_manual(values = palette_light()) +
    theme_tq() + labs(x="", y="")+
    theme(axis.text.x = element_text(angle = 75, vjust = 1, hjust = 1))
```

```{r}
augment(fit_lm) %>%
  ggplot(aes(x = date, y = .resid)) +
    geom_hline(yintercept = 0, color = "red") +
    geom_point(alpha = 0.5, color = palette_light()[[1]]) +
    geom_smooth() +
    theme_tq()
```
#Visualizing the Prediction test
```{r}
pred_test <- test %>%
  add_predictions(fit_lm, "pred_lm") %>%
  add_residuals(fit_lm, ".resid_lm")
```

```{r}
pred_test %>%
    ggplot(aes(x = date, y = .resid_lm)) +
    geom_hline(yintercept = 0, color = "red") +
    geom_point(alpha = 0.5, color = palette_light()[[1]]) +
    geom_smooth() +
    theme_tq()
```


```{r}
pred_test %>%
  gather(x, y, revenue, pred_lm) %>%
  ggplot(aes(x = date, y = y, color = x)) +
    geom_point(alpha = 0.5) +
    geom_line(alpha = 0.5) +
    # scale_color_manual(values = palette_light()) +
    theme_tq()

From October 01 to December 01 pred_lm shows the prediction test data. 
```


# Forcasting: We wil start from collecting time index data from DailyTransaction table and use tk_index() function

```{r}
# Extract index ====================================================
TimeIndex <- DailyTransaction %>%
    tk_index()

```


```{r }
TimeseriesTransaction %>%
  ggplot(aes(x = date, y = diff)) +
    geom_point(alpha = 0.5, aes(color = as.factor(diff))) +
    geom_line(alpha = 0.5) +
    #scale_color_manual(values = palette_light()) +
    theme_tq()
```


```{r }
TimeseriesTransaction %>%
  select(date, wday.lbl, diff) %>%
  filter(wday.lbl != "Sunday" & diff > 86400) %>%
  mutate(days_missing = diff / 86400 -1)
```

```{r Do I need these?}
off_days <- c("2010-12-24", "2010-12-25", "2010-12-26", "2010-12-27", "2010-12-28", "2010-12-29", "2010-12-30", "2010-01-01", "2010-01-02", "2010-01-03",
              "2011-04-22", "2011-04-23", "2011-04-24", "2011-04-25", "2011-05-02", "2011-05-30", "2011-08-29", "2011-04-29", "2011-04-30") %>% 
  ymd()
```

```{r}
future_time_index <- TimeIndex %>%
    tk_make_future_timeseries(n_future = 300, inspect_weekdays = TRUE, inspect_months = FALSE, skip_values = off_days)
future_time_index %>%
    tk_get_timeseries_signature() %>%
    ggplot(aes(x = index, y = diff)) +
    geom_point(alpha = 0.5, aes(color = as.factor(diff))) +
    geom_line(alpha = 0.5) +
    scale_color_manual(values = palette_light()) +
    theme_tq()
```

```{r }
future_data <- future_time_index %>%
    tk_get_timeseries_signature() %>%
    rename(date = index)

prediction <- predict(fit_lm, newdata = future_data)

prediction <- future_data %>%
    select(date) %>%
    add_column(revenue = prediction)

# ======================Something Wrong in Code Below. Picking up old variables. Need a fix.================
DailyTransaction %>%
  select(SalesDate, TotalRevenueFromSales) %>%
  rename(date = SalesDate, revenue = TotalRevenueFromSales) %>%
  rbind(prediction) %>%
  ggplot(aes(x = date, y = revenue)) +
    scale_x_date() +
    geom_vline(xintercept = as.numeric(max(DailyTransaction$SalesDate)), color = "red", size = 1) +
    geom_point(alpha = 0.5) +
    geom_line(alpha = 0.5) +
    theme_tq()
```
# ===========System Output==========================
# Error: Must subset columns with a valid subscript vector.
x Subscript has the wrong type `tbl_df<
  SalesDate          : date
  total.sales        : double
  mean.sales         : double
  total.quantity.sold: integer
  mean.quantity.sold : double
>`.
i It must be numeric or character.

 # ================================================

```{r}
residual_testing <- pred_test$.resid_lm

sd_of_res_testing <- sd(residual_testing, na.rm = TRUE)

prediction <- prediction %>%
    mutate(
        lo.95 = revenue - 1.96 * sd_of_res_testing,
        lo.80 = revenue - 1.28 * sd_of_res_testing,
        hi.80 = revenue + 1.28 * sd_of_res_testing,
        hi.95 = revenue + 1.96 * sd_of_res_testing
        )
```


```{r } 
DailyTransaction %>%
  select(SalesDate, TotalRevenueFromSales) %>%
  rename(date = SalesDate, revenue = TotalRevenueFromSales) %>%
  ggplot(aes(x = date, y = revenue)) +
    geom_point(alpha = 0.5) +
    geom_line(alpha = 0.5) +
    geom_ribbon(aes(ymin = lo.95, ymax = hi.95), data = prediction, 
                fill = "#D5DBFF", color = NA, size = 0) +
    geom_ribbon(aes(ymin = lo.80, ymax = hi.80, fill = key), data = prediction,
                fill = "#596DD5", color = NA, size = 0, alpha = 0.8) +
    geom_point(aes(x = date, y = revenue), data = prediction,
               alpha = 0.5, color = palette_light()[[2]]) +
    geom_smooth(aes(x = date, y = revenue), data = prediction,
                method = 'loess', color = "white") +
    theme_tq()
```

# Displaying prediction for 257 days starting from last transaction date of DailyTransaction (09December2011). The prediction is upto 2012-10-04